{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6370f16b",
   "metadata": {},
   "source": [
    "# Evaluate example for M3CAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "877918fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset,DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.cluster import KMeans\n",
    "from torch import nn\n",
    "import random\n",
    "\n",
    "import swin_util as swu\n",
    "from criterion import *\n",
    "import model.arch_util as arch_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c205668",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "setup_seed(1)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012536d7",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f2f51ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nextPermutation(nums):\n",
    "    if len(nums)<=1:\n",
    "        return \n",
    "    for i in range(len(nums)-2,-1,-1):\n",
    "        if nums[i]<nums[i+1]:\n",
    "            for k in range(len(nums)-1,i,-1):\n",
    "                if nums[k]>nums[i]:\n",
    "                    nums[i],nums[k]=nums[k],nums[i]\n",
    "                    nums[i+1:]=sorted(nums[i+1:])\n",
    "                    break\n",
    "            break\n",
    "        else:\n",
    "            if i==0:\n",
    "                nums.sort()\n",
    "def get_choice(index,number):\n",
    "    now_array = []\n",
    "    def recursion(index,num,start,now):\n",
    "        if num == 0:\n",
    "            now_array.append(now.copy())\n",
    "            return 0\n",
    "        else:\n",
    "            for i in range(start,len(index)):\n",
    "                now.append(index[i])\n",
    "                recursion(index,num-1,i+1,now)\n",
    "                now.pop()\n",
    "    recursion(index,number,0,[])\n",
    "    return now_array\n",
    "def compute_area(flows):\n",
    "    ## 构建所有的矩形\n",
    "    all_matrix = []\n",
    "    for i in range(len(flows)):\n",
    "        x,y = flows[i]\n",
    "        dx,dy = x+1,y+1\n",
    "        if dx<=2 and dy <=2:\n",
    "            all_matrix.append((x,y,dx,dy))\n",
    "        elif dx>2 and dy > 2:\n",
    "            all_matrix.append((x,y,2,2))\n",
    "        elif dx>2 and dy <= 2:\n",
    "            all_matrix.append((x,y,2,dy))\n",
    "            all_matrix.append((0,y,dx-2,dy))\n",
    "        else:\n",
    "            all_matrix.append((x,y,dx,2))\n",
    "            all_matrix.append((x,0,dx,dy-2))\n",
    "    \n",
    "    ## 然后计算所有面积\n",
    "    ## 每个矩形 左上 x y + 右下 x y\n",
    "    ps = []\n",
    "    for info in all_matrix:\n",
    "        ps.append(info[0])\n",
    "        ps.append(info[2])\n",
    "#         print(info)\n",
    "    ps.sort()\n",
    "    ans = 0\n",
    "    for i in range(1,len(ps)):\n",
    "        a, b = ps[i - 1], ps[i]\n",
    "        width = b - a\n",
    "        if width == 0:\n",
    "            continue\n",
    "        lines = [(info[1], info[3]) for info in all_matrix if info[0] <= a and b <= info[2]]\n",
    "        lines.sort()\n",
    "        height, l, r = 0, -1, -1\n",
    "        for cur in lines:\n",
    "            if cur[0] > r:\n",
    "                height += r - l\n",
    "                l, r = cur\n",
    "            elif cur[1] > r:\n",
    "                r = cur[1]\n",
    "        height += r - l\n",
    "        ans += height * width\n",
    "    return ans\n",
    "def select_best(flows,number):\n",
    "    flows = flows % 2\n",
    "#     print(flows.shape)\n",
    "    flows[0] = 0\n",
    "    index = [i for i in range(flows.shape[0]) if i !=0]\n",
    "    # 递归得到所有可能的组合\n",
    "    all_choice = get_choice(index,number)\n",
    "    minIndex = 0\n",
    "    minArea  = 0\n",
    "    for i in range(len(all_choice)):\n",
    "        choice_now = all_choice[i] + [0]\n",
    "        area = compute_area(flows[choice_now])\n",
    "#         print(area)\n",
    "        if minArea > area:\n",
    "            minArea = area\n",
    "            minIndex = i\n",
    "    return [0]+all_choice[minIndex]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "963a6b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_raw = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "class RealDataset(Dataset):\n",
    "    def __init__(self,path):\n",
    "        self.image_path = path\n",
    "        self.image_list = os.listdir(self.image_path)\n",
    "        \n",
    "        self.data_path_list = []\n",
    "        self.capture_path   = []\n",
    "        \n",
    "        for img_path in self.image_list:\n",
    "            img = os.path.join(self.image_path,img_path)\n",
    "            self.capture_path.append(img)\n",
    "            part_name = os.listdir(img)\n",
    "            for part in part_name:\n",
    "                part_path = os.path.join(img,part)\n",
    "                if os.path.isdir(part_path):\n",
    "                    self.data_path_list.append(part_path)\n",
    "        self.nframes = 4\n",
    "        self.choice_list = self.getChoice()\n",
    "        \n",
    "    def getChoice(self):\n",
    "        choice_list = []\n",
    "        for i in tqdm(range(len(self.data_path_list))):\n",
    "            path = self.data_path_list[i]\n",
    "            dir_path = os.path.dirname(path)\n",
    "\n",
    "            data = {}\n",
    "            image_flows = []\n",
    "            for i in range(20):\n",
    "                flows = np.load(os.path.join(path,\"offset\"+str(i)+\".npy\"))\n",
    "                flows = torch.FloatTensor(flows)\n",
    "                image_flows.append(flows)\n",
    "            # 接下来筛选\n",
    "            image_flows = torch.stack(image_flows,dim=0)\n",
    "            flows_mean = torch.mean(image_flows[:,:,14:-14,14:-14],dim=[2,3])\n",
    "            select_choice = select_best(flows_mean,self.nframes - 1)\n",
    "            choice_list.append(select_choice)\n",
    "        return choice_list\n",
    "        \n",
    "    def __getitem__(self,index):\n",
    "        path = self.data_path_list[index]\n",
    "        dir_path = os.path.dirname(path)\n",
    "        \n",
    "        data = {}\n",
    "        image_burst = []\n",
    "        image_flows = []\n",
    "        for i in range(20):\n",
    "            burst = np.load(os.path.join(path,str(i)+'.npy'))\n",
    "            burst = transform_raw(burst)\n",
    "            image_burst.append(burst)\n",
    "            flows = np.load(os.path.join(path,\"offset\"+str(i)+\".npy\"))\n",
    "            flows = torch.FloatTensor(flows)\n",
    "            image_flows.append(flows)\n",
    "        # 接下来筛选\n",
    "        image_flows = torch.stack(image_flows,dim=0)\n",
    "        image_burst = torch.stack(image_burst,dim=0)\n",
    "        \n",
    "        select_choice = self.choice_list[index]\n",
    "        \n",
    "        data[\"burst\"] = image_burst[select_choice]  #(4,4,56,56) (N,C,H,W)\n",
    "        data[\"flows\"] = image_flows[select_choice]\n",
    "        image_frame = np.load(os.path.join(path,'original'+str(0)+'.npy'))\n",
    "        image_frame = transform_raw(image_frame)\n",
    "        data['frame_gt'] = image_frame\n",
    "        meta_info = np.load(os.path.join(dir_path,'meta_info'+str(0)+'.npy'),allow_pickle=True).item()\n",
    "        data['meta_info'] = meta_info\n",
    "        return data\n",
    "    def __len__(self):\n",
    "        return len(self.data_path_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be6b549f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dd5b2d4f20144e68ed87ab211a1540d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/52 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "imgpath = \"./dataset/\"\n",
    "testDataset = RealDataset(imgpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3709ba04",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e0301dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class M3CAM(nn.Module):\n",
    "    def __init__(self, num_in_ch = 4, embed_dim = 96, ape = False, drop_rate = 0., num_feat = 64):\n",
    "        \n",
    "        super(M3CAM, self).__init__()\n",
    "        embed_dim = 96\n",
    "        self.pixel_shuffle = nn.PixelShuffle(2)\n",
    "        \n",
    "        self.upconv0 = nn.Conv2d(num_in_ch, embed_dim, 3, 1, 1, bias=True)\n",
    "        self.upconv1 = nn.Conv2d(embed_dim, embed_dim * 4, 3, 1, 1, bias=True)\n",
    "        self.upconv2 = nn.Conv2d(embed_dim, embed_dim, 3, 1, 1, bias=True)\n",
    "        self.upconv3 = nn.Conv2d(embed_dim, num_feat, 3, 1, 1, bias=True)\n",
    "        self.lrelu = nn.LeakyReLU(0.1,inplace=True)\n",
    "        \n",
    "        nframes = 4\n",
    "        depths=[6, 6, 6, 6]\n",
    "        num_heads=[6, 6, 6, 6]\n",
    "        mlp_ratio = 2.0\n",
    "        qkv_bias = True\n",
    "        qk_scale = None\n",
    "        drop_rate = 0.\n",
    "        attn_drop_rate = 0.\n",
    "        drop_path_rate = 0.1\n",
    "        norm_layer = nn.LayerNorm\n",
    "        \n",
    "        img_size = 56\n",
    "        img_size_1 = 112\n",
    "        img_size_2 = 224\n",
    "        patch_size = 4\n",
    "        use_checkpoint=False\n",
    "        window_size = 7\n",
    "        \n",
    "        self.patch_norm = True\n",
    "        self.ape = ape        \n",
    "        self.norm_layer = norm_layer\n",
    "        self.num_features = embed_dim\n",
    "        self.norm = self.norm_layer(self.num_features)\n",
    "        \n",
    "        self.num_layers = 1 #len(depths)\n",
    "        \n",
    "        \n",
    "        self.patch_embed = swu.PatchEmbed(\n",
    "            img_size=img_size, patch_size=patch_size, in_chans=embed_dim, embed_dim=embed_dim,\n",
    "            norm_layer=norm_layer if self.patch_norm else None)\n",
    "        num_patches = self.patch_embed.num_patches # 14 * 14 \n",
    "        patches_resolution = self.patch_embed.patches_resolution #（14，14）\n",
    "        \n",
    "        num_patches_1 = num_patches * 4 # 28 * 28 = 784\n",
    "        patches_resolution_1 = patches_resolution * 2 # 28 ,28\n",
    "        \n",
    "        num_patches_2 = num_patches_1 * 2 # 56 * 56 = 784*4\n",
    "        patches_resolution_2 = patches_resolution_1 * 2 # 56,56\n",
    "        \n",
    "        self.patch_unembed = swu.PatchUnEmbed(\n",
    "            img_size=img_size, patch_size=patch_size, in_chans=embed_dim, embed_dim=embed_dim,\n",
    "            norm_layer=norm_layer if self.patch_norm else None)\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))] \n",
    "        self.swin_layers = nn.ModuleList()\n",
    "        for i_layer in range(depths[0]):\n",
    "            layer = swu.SwinTransformerBlock(dim=embed_dim,\n",
    "                                             input_resolution=(patches_resolution[0]//2,patches_resolution[1]//2),\n",
    "                                             num_heads=num_heads[0],\n",
    "                                             window_size=window_size,\n",
    "                                             shift_size=0 if (i_layer % 2 == 0) else window_size // 2, # 一层fix + 一层 shift\n",
    "                                             mlp_ratio = mlp_ratio,\n",
    "                                             qkv_bias = qkv_bias,\n",
    "                                             qk_scale = qk_scale,   \n",
    "                                             drop = drop_rate,\n",
    "                                             attn_drop = attn_drop_rate,\n",
    "                                             drop_path = dpr[i_layer],\n",
    "                                             norm_layer = norm_layer\n",
    "                                             )\n",
    "            self.swin_layers.append(layer)\n",
    "        self.pre_norm = norm_layer(embed_dim)\n",
    "\n",
    "        # PatchEmbed 将 x 从维度2 开始拉直 x (B,96,112,112) -> (B.96,112*112)\n",
    "        dpr_1 = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))] \n",
    "        \n",
    "        self.up_swin_1 = nn.ModuleList()\n",
    "        for i_layer in range(self.num_layers):\n",
    "            # swin trans 提取特征\n",
    "            layer = swu.RSTB(dim=embed_dim,\n",
    "                     input_resolution=(patches_resolution_1[0],\n",
    "                                       patches_resolution_1[1]),\n",
    "                     depth=depths[i_layer],\n",
    "                     num_heads=num_heads[i_layer],\n",
    "                     window_size=window_size,\n",
    "                     mlp_ratio=mlp_ratio,\n",
    "                     qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                     drop=drop_rate, attn_drop=attn_drop_rate,\n",
    "                     drop_path=dpr_1[sum(depths[:i_layer]):sum(depths[:i_layer + 1])],  # no impact on SR results\n",
    "                     norm_layer=norm_layer,\n",
    "                     downsample=None,\n",
    "                     use_checkpoint=use_checkpoint,\n",
    "                     img_size=img_size_1,\n",
    "                     patch_size=patch_size)\n",
    "            self.up_swin_1.append(layer)\n",
    "\n",
    "\n",
    "        dpr_2 = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))] \n",
    "        self.up_swin_2 = nn.ModuleList()\n",
    "        for i_layer in range(self.num_layers):\n",
    "            # swin trans 提取特征\n",
    "            layer = swu.RSTB(dim=embed_dim,\n",
    "                     input_resolution=(patches_resolution_2[0],\n",
    "                                       patches_resolution_2[1]),\n",
    "                     depth=depths[i_layer],\n",
    "                     num_heads=num_heads[i_layer],\n",
    "                     window_size=window_size,\n",
    "                     mlp_ratio=mlp_ratio,\n",
    "                     qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                     drop=drop_rate, attn_drop=attn_drop_rate,\n",
    "                     drop_path=dpr_2[sum(depths[:i_layer]):sum(depths[:i_layer + 1])],  # no impact on SR results\n",
    "                     norm_layer=norm_layer,\n",
    "                     downsample=None,\n",
    "                     use_checkpoint=use_checkpoint,\n",
    "                     img_size=img_size_2,\n",
    "                     patch_size=patch_size)\n",
    "            self.up_swin_2.append(layer)\n",
    "        \n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "        \n",
    "        self.Fusion = nn.Conv2d(embed_dim*nframes, embed_dim, 1, 1, padding=0, bias=True)\n",
    "        \n",
    "        self.upsample1 = nn.ConvTranspose2d(embed_dim,embed_dim,3, 2, 1,output_padding=1, bias=True)\n",
    "        self.upsample2 = nn.ConvTranspose2d(num_feat,num_feat,3, 2, 1,output_padding=1, bias=True)\n",
    "        \n",
    "        self.convHR = nn.Conv2d(num_feat, num_feat, 1, 1, padding=0, bias=True)\n",
    "        self.convLast = nn.Conv2d(num_feat, 1, 1, 1, padding=0, bias=True)\n",
    "        \n",
    "    def forward_features(self,x):\n",
    "        # x_size (56,56)\n",
    "        x_size = (x.shape[-2],x.shape[-1])\n",
    "        # x (B,96,56,56) -> (B.56*56,96) 长度为56*56 特征 96 的序列\n",
    "        x = self.patch_embed(x,use_norm=True)\n",
    "        x = self.pos_drop(x)\n",
    "        for idx,layer in enumerate(self.swin_layers):\n",
    "            x = layer(x,x_size)\n",
    "        x = self.pre_norm(x) #(BN,56*56,96)\n",
    "        x = self.patch_unembed(x,x_size) # 变回 (BN,96,56,56)\n",
    "        return x\n",
    "    def up_sample_1(self,x):\n",
    "        #(B,96,112,112)\n",
    "        x_size = (x.shape[-2], x.shape[-1])\n",
    "        x = self.patch_embed(x) #(B,112*112,96)\n",
    "        x = self.pos_drop(x) #(B,112*112,96)\n",
    "        for idx,layer in enumerate(self.up_swin_1):\n",
    "            x = layer(x, x_size)\n",
    "        x = self.norm(x) #(B,112*112,96)\n",
    "        x = self.patch_unembed(x,x_size)  #(B,96,112,112)\n",
    "        return x\n",
    "    def up_sample_2(self,x):\n",
    "        #(B,96,224,224)\n",
    "        x_size = (x.shape[-2], x.shape[-1])\n",
    "        x = self.patch_embed(x) #(B,224,224,96)\n",
    "        x = self.pos_drop(x) #(B,224,224,96)\n",
    "        for idx,layer in enumerate(self.up_swin_2):\n",
    "            x = layer(x, x_size)\n",
    "        x = self.norm(x) #(B,224,224,96)\n",
    "        x = self.patch_unembed(x,x_size)  #(B,96,224,224)\n",
    "        return x\n",
    "    def forward(self,x,offset):\n",
    "        B,N,C,H,W = x.size() # x (B,N,4,56,56)\n",
    "        # 升维\n",
    "        x   = x.view(B*N,C,H,W)\n",
    "        img = self.lrelu(self.upconv0(x))  # img (B*N,96,56,56)\n",
    "        # img1 (B,96,56,56)\n",
    "        # 特征提取\n",
    "        img = self.forward_features(img)   # img (B*N,96,56,56)\n",
    "        img = self.lrelu(self.pixel_shuffle(self.upconv1(img)))  # img (B*N,96,112,112)\n",
    "        \n",
    "        _,_,H,W = img.size()               \n",
    "        # 对齐\n",
    "        \n",
    "        offset = offset.view(B*N,-1,H,W) # offsets(B*N,2,H,W)\n",
    "        \n",
    "        warp_fea = arch_util.flow_warp(img,offset.permute(0,2,3,1),'bilinear')  # warp_fea (B*N,96,112,112)\n",
    "        warp_fea = warp_fea.view(B,N,-1,H,W)  # warp_fea (B,N,96,112,112)\n",
    "        \n",
    "        warp_fea[:,0,:,:,:] = img.view(B,N,-1,H,W)[:,0,:,:,:]\n",
    "        \n",
    "        warp_fea = warp_fea.view(B,-1,H,W)    # warp_fea (B,96*N,112,112)\n",
    "        warp_fea = self.lrelu(self.Fusion(warp_fea))  # warp_fea (B,96,112,112)\n",
    "        \n",
    "        # swin Transformer\n",
    "        features = self.up_sample_1(warp_fea)  # features (B,96,112,112)\n",
    "        # 升维,放大\n",
    "        features = self.lrelu(self.upsample1(self.upconv2(features))) # features (B,96,224,224)\n",
    "        # swin Transformer\n",
    "        features = self.up_sample_2(features) # features (B,96,224,224)\n",
    "        # 放大，降维\n",
    "        features = self.lrelu(self.upsample2(self.upconv3(features))) # features (B,64,448,448)\n",
    "        # 降维\n",
    "        features = self.lrelu(self.convHR(features)) # features (B,64,448,448)\n",
    "        \n",
    "        output = self.convLast(features)  # features (B,1,448,448)\n",
    "        # upsample \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "204ae4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = M3CAM().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c7e85f",
   "metadata": {},
   "source": [
    "## raw 2 rgb processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e66e7a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_raw(image,meta_info):\n",
    "    image = image.clamp(0.0, 1.0)\n",
    "    image = image.permute(1,2,0)\n",
    "    image = image.numpy()\n",
    "    def f(x):\n",
    "        return x / 4\n",
    "    black_level = np.array(meta_info['black_level'][0:2] + meta_info['black_level'][-1:])\n",
    "    \n",
    "    \n",
    "    image = np.rint(f(image*1024)).astype(np.uint8)\n",
    "    black_level = np.rint(f(np.array(black_level, dtype=np.uint8)))\n",
    "    max_image = meta_info[\"max_image\"]\n",
    "    min_image = meta_info[\"min_image\"]\n",
    "\n",
    "    image = np.clip(np.rint((image - min_image) / (max_image - min_image) * 255.0), 0.0, 255.0).astype(np.uint8)\n",
    "    black_level = np.clip(np.rint((black_level - min_image) / (max_image - min_image) * 255.0), 0.0, 255.0).astype(np.uint8)\n",
    "    image -= black_level[0]\n",
    "    image = image.astype(np.uint8)\n",
    "    im_dem_np = cv2.cvtColor(image, cv2.COLOR_BayerRG2BGR)\n",
    "    image = im_dem_np / 255.0\n",
    "    wb = meta_info['wb']\n",
    "    ave = meta_info[\"ave\"]\n",
    "    mid = meta_info[\"mid\"]\n",
    "    bright = meta_info[\"bright\"]\n",
    "\n",
    "\n",
    "    gains = np.array([wb[0],wb[1],wb[2]]) * bright\n",
    "    image[:,:,0] = image[:,:,0]  * gains[0]\n",
    "    image[:,:,1] = image[:,:,1]  * gains[1]\n",
    "    image[:,:,2] = image[:,:,2]  * gains[2]\n",
    "    image[image<=0.0] = 0.0\n",
    "    image[image>=1.0] = 1.0\n",
    "    image[image<=1e-8] = 1e-8\n",
    "    image = image**(1.0/2.2)\n",
    "    image = 3 * image**2 - 2 * image**3\n",
    "    def color_scale_display(image, input_, output_):\n",
    "        shadow, midtones, highlight = input_\n",
    "        outShadow, outHighlight = output_\n",
    "        diff = highlight - shadow\n",
    "        imageDiff = np.maximum(image - shadow, 0.0)\n",
    "        clImage = np.power(imageDiff / diff, 1. / midtones)\n",
    "        outImage = clImage * (outHighlight - outShadow) + outShadow\n",
    "        image = outImage\n",
    "        image[image <= 0.0] = 0.0\n",
    "        image[image >= 1.0] = 1.0\n",
    "        return image\n",
    "\n",
    "    im_rgb = np.zeros_like(image)\n",
    "    im_rgb[:, :, 0] = color_scale_display(image[:, :, 0], [16.0/255., 1., 255./255.], [0., 1.])\n",
    "    im_rgb[:, :, 1] = color_scale_display(image[:, :, 1], [15.0/255., 1., 221./255.], [0., 1.])\n",
    "    im_rgb[:, :, 2] = color_scale_display(image[:, :, 2], [15.0/255., 1., 215./255.], [0., 1.])\n",
    "\n",
    "    im_rgb = torch.from_numpy(im_rgb).float().permute(2, 0, 1)\n",
    "    return im_rgb\n",
    "class UnNormalize(object):\n",
    "    def __init__(self,mean,std):\n",
    "        self.mean=mean\n",
    "        self.std=std\n",
    " \n",
    "    def __call__(self,tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        :param tensor: tensor image of size (B,C,H,W) to be un-normalized\n",
    "        :return: UnNormalized image\n",
    "        \"\"\"\n",
    "        res = torch.zeros_like(tensor).to(device)\n",
    "        for i,d  in enumerate(zip(tensor,self.mean,self.std)):\n",
    "            t, m, s = d\n",
    "            res[i] = t.mul(s).add(m)\n",
    "        return res\n",
    "mean_list = (0.5,)\n",
    "std_list = (0.5,)\n",
    "unnorm = UnNormalize(mean_list, std_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a40cf4",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b19c56ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor():\n",
    "    def __init__(self, net, objective, loss_weight=None):\n",
    "        super().__init__()\n",
    "        if loss_weight is None:\n",
    "            loss_weight = {'rgb': 1.0}\n",
    "        self.loss_weight = loss_weight\n",
    "        self.net = net\n",
    "        self.objective = objective\n",
    "\n",
    "    def __call__(self, data, isTrain):\n",
    "        pred = self.net(data['burst'],data['flows'])\n",
    "        pred = pred.clamp(0.0,1.0)\n",
    "        loss_rgb_raw = self.objective['rgb'](pred, data['frame_gt'])\n",
    "        loss_rgb = self.loss_weight['rgb'] * loss_rgb_raw\n",
    "        loss = loss_rgb\n",
    "        stats = {'Loss/rgb': loss_rgb.item(),\n",
    "                 'Loss/raw/rgb': loss_rgb_raw.item()}\n",
    "        if isTrain:\n",
    "            pass\n",
    "        else:\n",
    "            pred_process = pred.detach().clone()\n",
    "            pred_process = pred_process.clamp(0.0,1.0)\n",
    "            pred_process = process_raw(pred_process[0].to('cpu'),data['meta_info']).to(device)\n",
    "            frame_gt = data['frame_gt']\n",
    "            frame_gt = process_raw(frame_gt[0].to('cpu'),data['meta_info']).to(device)\n",
    "            if 'psnr' in self.objective.keys():\n",
    "                psnr =  self.objective['psnr'](pred_process, frame_gt)\n",
    "                stats['Stat/psnr'] = psnr.item()\n",
    "            if 'ssim' in self.objective.keys():\n",
    "                ssim =  self.objective['ssim'](pred_process, frame_gt, valid=None)\n",
    "                stats['Stat/ssim'] = 1 - ssim.item()\n",
    "            if 'lpips' in self.objective.keys():\n",
    "                lpips =  self.objective['lpips'](pred_process, frame_gt, valid=None)\n",
    "                stats['Stat/lpips'] = lpips.item()\n",
    "        stats['Loss/total'] = loss.item()\n",
    "        return loss, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f8af2cca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: D:\\Users\\22496\\anaconda3\\envs\\pytorch\\lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n"
     ]
    }
   ],
   "source": [
    "criterion = {'rgb': PixelWiseError(metric='l1', boundary_ignore=56).to(device), 'psnr': PSNR(boundary_ignore=56).to(device),'ssim': SSIM(boundary_ignore=56).to(device),'lpips':LPIPS(boundary_ignore=56).to(device)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "27bc1030",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"./models/SR.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3f091eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_weight = {'rgb': 1.0}\n",
    "actor = Actor(net=model, objective=criterion, loss_weight=loss_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "817c1bbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b311ada267554ec5a3ef2d401843ddc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/52 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:0.006316702822984483，PSNR:38.102121573228104,SSIM:0.917108797110044,LPIPS:0.069236967802191\n"
     ]
    }
   ],
   "source": [
    "Loss = 0.0\n",
    "psnr = 0.0\n",
    "ssim = 0.0\n",
    "lpips = 0.0\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(len(testDataset))):\n",
    "        data = testDataset[i]\n",
    "        data['burst'] = data['burst'].unsqueeze(0).to(device)\n",
    "        data['flows'] = data['flows'].unsqueeze(0).to(device)\n",
    "        data['frame_gt'] = data['frame_gt'].unsqueeze(0).to(device)\n",
    "        loss, stats = actor(data,False)\n",
    "        psnr += stats['Stat/psnr']\n",
    "        ssim += stats['Stat/ssim']\n",
    "        lpips += stats['Stat/lpips']\n",
    "        Loss += loss.item()\n",
    "Loss /= len(testDataset)\n",
    "psnr /= len(testDataset)\n",
    "ssim /= len(testDataset)\n",
    "lpips /= len(testDataset)\n",
    "print(\"Loss:{}，PSNR:{},SSIM:{},LPIPS:{}\".format(Loss,psnr,ssim,lpips))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fd4086",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
